# Story 1.7: Monitoring & Observability

## Overview
Implement comprehensive monitoring and observability using OpenTelemetry (OTel) for the entire FLRTS stack, including Supabase, Edge Functions, n8n workflows, and Redis queues.

## Context
- **Epic**: Epic 1 - Infrastructure & Core Systems
- **Priority**: P1 (Important but less urgent)
- **Dependencies**:
  - Story 1.1 (Supabase setup) - COMPLETED
  - Story 1.6 (Redis Queue) - COMPLETED
- **Blocks**: Production deployment readiness

## Technical Requirements

### OpenTelemetry Stack Setup
```typescript
// Environment variables for OTel configuration
interface ObservabilityConfig {
  // OTel Collector endpoint
  OTEL_EXPORTER_OTLP_ENDPOINT: string;     // "http://otel-collector:4318"
  OTEL_EXPORTER_OTLP_PROTOCOL: string;     // "http/protobuf"

  // Service identification
  OTEL_SERVICE_NAME: string;               // "flrts-production"
  OTEL_SERVICE_VERSION: string;            // "1.0.0"
  OTEL_DEPLOYMENT_ENVIRONMENT: string;     // "production"

  // Resource attributes
  OTEL_RESOURCE_ATTRIBUTES: string;        // "service.namespace=flrts,team=engineering"

  // Sampling configuration
  OTEL_TRACES_SAMPLER: string;             // "parentbased_traceidratio"
  OTEL_TRACES_SAMPLER_ARG: string;         // "0.1" (10% sampling)
}
```

### 1. Supabase Edge Functions Instrumentation
```typescript
// Edge Function with OTel tracing
import { trace, context, SpanStatusCode } from '@opentelemetry/api';
import { NodeTracerProvider } from '@opentelemetry/sdk-trace-node';
import { Resource } from '@opentelemetry/resources';
import { SemanticResourceAttributes } from '@opentelemetry/semantic-conventions';
import { OTLPTraceExporter } from '@opentelemetry/exporter-trace-otlp-http';
import { BatchSpanProcessor } from '@opentelemetry/sdk-trace-base';

// Initialize tracer
const provider = new NodeTracerProvider({
  resource: new Resource({
    [SemanticResourceAttributes.SERVICE_NAME]: 'flrts-edge-functions',
    [SemanticResourceAttributes.SERVICE_VERSION]: process.env.FUNCTION_VERSION || '1.0.0',
    [SemanticResourceAttributes.DEPLOYMENT_ENVIRONMENT]: process.env.ENVIRONMENT || 'development',
  }),
});

const exporter = new OTLPTraceExporter({
  url: `${process.env.OTEL_EXPORTER_OTLP_ENDPOINT}/v1/traces`,
  headers: {
    'api-key': process.env.OTEL_API_KEY || '',
  },
});

provider.addSpanProcessor(new BatchSpanProcessor(exporter));
provider.register();

const tracer = trace.getTracer('flrts-edge-functions');

// Instrumented Edge Function
export default async function handler(req: Request): Promise<Response> {
  // Start root span
  return tracer.startActiveSpan('edge-function-handler', async (span) => {
    try {
      // Add request attributes
      span.setAttributes({
        'http.method': req.method,
        'http.url': req.url,
        'http.target': new URL(req.url).pathname,
        'user.id': req.headers.get('x-user-id') || 'anonymous',
      });

      // Process request with nested spans
      const result = await tracer.startActiveSpan('process-request', async (childSpan) => {
        childSpan.setAttribute('request.body.size', req.headers.get('content-length') || 0);

        // Your business logic here
        const response = await processRequest(req);

        childSpan.setStatus({ code: SpanStatusCode.OK });
        return response;
      });

      // Record success metrics
      span.setAttributes({
        'http.status_code': result.status,
        'response.size': result.headers.get('content-length') || 0,
      });

      span.setStatus({ code: SpanStatusCode.OK });
      return result;

    } catch (error) {
      // Record error
      span.recordException(error as Error);
      span.setStatus({
        code: SpanStatusCode.ERROR,
        message: error instanceof Error ? error.message : 'Unknown error',
      });

      return new Response('Internal Server Error', { status: 500 });
    } finally {
      span.end();
    }
  });
}

// Custom metrics collection
import { metrics } from '@opentelemetry/api';
import { MeterProvider } from '@opentelemetry/sdk-metrics';
import { PeriodicExportingMetricReader } from '@opentelemetry/sdk-metrics';
import { OTLPMetricExporter } from '@opentelemetry/exporter-metrics-otlp-http';

const metricExporter = new OTLPMetricExporter({
  url: `${process.env.OTEL_EXPORTER_OTLP_ENDPOINT}/v1/metrics`,
});

const meterProvider = new MeterProvider({
  readers: [
    new PeriodicExportingMetricReader({
      exporter: metricExporter,
      exportIntervalMillis: 10000, // Export every 10 seconds
    }),
  ],
});

const meter = meterProvider.getMeter('flrts-metrics');

// Create custom metrics
const requestCounter = meter.createCounter('http_requests_total', {
  description: 'Total number of HTTP requests',
});

const requestDuration = meter.createHistogram('http_request_duration_ms', {
  description: 'HTTP request duration in milliseconds',
});

const activeConnections = meter.createUpDownCounter('active_connections', {
  description: 'Number of active connections',
});
```

### 2. PostgreSQL Monitoring Configuration
```yaml
# pg_stat_statements configuration (postgresql.conf)
shared_preload_libraries = 'pg_stat_statements'
pg_stat_statements.track = all
pg_stat_statements.max = 10000
pg_stat_statements.track_utility = on

# Enable logging for slow queries
log_min_duration_statement = 100  # Log queries slower than 100ms
log_statement = 'all'
log_duration = on
log_line_prefix = '%t [%p]: [%l-1] user=%u,db=%d,app=%a,client=%h '
```

```sql
-- Create monitoring views
CREATE OR REPLACE VIEW monitoring.database_metrics AS
SELECT
  current_database() as database_name,
  numbackends as active_connections,
  xact_commit as transactions_committed,
  xact_rollback as transactions_rolled_back,
  blks_read as blocks_read,
  blks_hit as blocks_hit,
  tup_returned as rows_returned,
  tup_fetched as rows_fetched,
  tup_inserted as rows_inserted,
  tup_updated as rows_updated,
  tup_deleted as rows_deleted,
  conflicts as replication_conflicts,
  deadlocks,
  checksum_failures,
  EXTRACT(EPOCH FROM (now() - stats_reset)) as seconds_since_reset
FROM pg_stat_database
WHERE datname = current_database();

-- Query performance monitoring
CREATE OR REPLACE VIEW monitoring.slow_queries AS
SELECT
  queryid,
  query,
  calls,
  total_exec_time,
  mean_exec_time,
  max_exec_time,
  stddev_exec_time,
  rows,
  100.0 * shared_blks_hit / NULLIF(shared_blks_hit + shared_blks_read, 0) AS cache_hit_ratio
FROM pg_stat_statements
WHERE mean_exec_time > 100  -- Queries averaging over 100ms
ORDER BY mean_exec_time DESC
LIMIT 20;

-- Table bloat monitoring
CREATE OR REPLACE VIEW monitoring.table_bloat AS
SELECT
  schemaname,
  tablename,
  pg_size_pretty(pg_total_relation_size(schemaname||'.'||tablename)) AS total_size,
  pg_size_pretty(pg_relation_size(schemaname||'.'||tablename)) AS table_size,
  pg_size_pretty(pg_total_relation_size(schemaname||'.'||tablename) - pg_relation_size(schemaname||'.'||tablename)) AS bloat_size,
  ROUND(100 * (pg_total_relation_size(schemaname||'.'||tablename) - pg_relation_size(schemaname||'.'||tablename)) / pg_total_relation_size(schemaname||'.'||tablename)::numeric, 2) AS bloat_percentage
FROM pg_tables
WHERE schemaname NOT IN ('pg_catalog', 'information_schema')
ORDER BY pg_total_relation_size(schemaname||'.'||tablename) DESC
LIMIT 20;
```

### 3. n8n Workflow Monitoring
```typescript
// n8n custom node for OTel instrumentation
import { INodeType, INodeTypeDescription, IExecuteFunctions } from 'n8n-workflow';
import { trace, SpanKind } from '@opentelemetry/api';

export class OtelInstrumentedNode implements INodeType {
  description: INodeTypeDescription = {
    displayName: 'OTel Instrumented Node',
    name: 'otelInstrumentedNode',
    group: ['transform'],
    version: 1,
    description: 'Node with OpenTelemetry instrumentation',
    defaults: {
      name: 'OTel Node',
    },
    inputs: ['main'],
    outputs: ['main'],
    properties: [],
  };

  async execute(this: IExecuteFunctions) {
    const tracer = trace.getTracer('n8n-workflows');

    return tracer.startActiveSpan('n8n-workflow-execution', {
      kind: SpanKind.INTERNAL,
      attributes: {
        'workflow.id': this.getWorkflow().id,
        'workflow.name': this.getWorkflow().name,
        'node.name': this.getNode().name,
        'node.type': this.getNode().type,
      },
    }, async (span) => {
      try {
        const items = this.getInputData();

        // Record input metrics
        span.setAttribute('input.items.count', items.length);

        // Process items with timing
        const startTime = Date.now();
        const processedItems = await this.processItems(items);
        const duration = Date.now() - startTime;

        // Record processing metrics
        span.setAttributes({
          'output.items.count': processedItems.length,
          'processing.duration_ms': duration,
          'processing.rate': processedItems.length / (duration / 1000),
        });

        span.setStatus({ code: SpanStatusCode.OK });
        return [processedItems];

      } catch (error) {
        span.recordException(error as Error);
        span.setStatus({
          code: SpanStatusCode.ERROR,
          message: error instanceof Error ? error.message : 'Unknown error',
        });
        throw error;
      } finally {
        span.end();
      }
    });
  }
}

// n8n webhook metrics collection
const n8nMetrics = {
  webhookReceived: meter.createCounter('n8n_webhook_received_total'),
  webhookDuration: meter.createHistogram('n8n_webhook_duration_ms'),
  workflowExecutions: meter.createCounter('n8n_workflow_executions_total'),
  workflowErrors: meter.createCounter('n8n_workflow_errors_total'),
  nodeExecutions: meter.createCounter('n8n_node_executions_total'),
};
```

### 4. Redis Queue Monitoring
```typescript
import { Queue, QueueEvents, Worker } from 'bullmq';
import { trace, metrics } from '@opentelemetry/api';

// Queue metrics
const queueMetrics = {
  jobsAdded: meter.createCounter('redis_queue_jobs_added_total'),
  jobsCompleted: meter.createCounter('redis_queue_jobs_completed_total'),
  jobsFailed: meter.createCounter('redis_queue_jobs_failed_total'),
  jobDuration: meter.createHistogram('redis_queue_job_duration_ms'),
  queueSize: meter.createObservableGauge('redis_queue_size'),
  activeJobs: meter.createObservableGauge('redis_queue_active_jobs'),
};

// Instrumented queue
class InstrumentedQueue extends Queue {
  async add(name: string, data: any, opts?: any) {
    return tracer.startActiveSpan(`queue.add.${name}`, async (span) => {
      span.setAttributes({
        'queue.name': this.name,
        'job.name': name,
        'job.data.size': JSON.stringify(data).length,
      });

      const job = await super.add(name, data, opts);

      queueMetrics.jobsAdded.add(1, {
        queue: this.name,
        job_name: name,
      });

      span.setAttribute('job.id', job.id);
      span.end();
      return job;
    });
  }
}

// Instrumented worker
class InstrumentedWorker extends Worker {
  constructor(name: string, processor: any, opts?: any) {
    const wrappedProcessor = async (job: any) => {
      return tracer.startActiveSpan(`worker.process.${job.name}`, async (span) => {
        const startTime = Date.now();

        span.setAttributes({
          'queue.name': name,
          'job.id': job.id,
          'job.name': job.name,
          'job.attempt': job.attemptsMade,
        });

        try {
          const result = await processor(job);
          const duration = Date.now() - startTime;

          queueMetrics.jobsCompleted.add(1, {
            queue: name,
            job_name: job.name,
          });

          queueMetrics.jobDuration.record(duration, {
            queue: name,
            job_name: job.name,
            status: 'completed',
          });

          span.setStatus({ code: SpanStatusCode.OK });
          return result;

        } catch (error) {
          queueMetrics.jobsFailed.add(1, {
            queue: name,
            job_name: job.name,
            error: error instanceof Error ? error.name : 'Unknown',
          });

          span.recordException(error as Error);
          span.setStatus({
            code: SpanStatusCode.ERROR,
            message: error instanceof Error ? error.message : 'Unknown error',
          });
          throw error;

        } finally {
          span.end();
        }
      });
    };

    super(name, wrappedProcessor, opts);
  }
}

// Queue health monitoring
async function monitorQueueHealth() {
  const queues = [
    'flrts:tasks:create',
    'flrts:reminders:schedule',
    'flrts:telegram:outbound',
  ];

  for (const queueName of queues) {
    const queue = new Queue(queueName);
    const counts = await queue.getJobCounts();

    // Record queue metrics
    queueMetrics.queueSize.addCallback(async (observableResult) => {
      observableResult.observe(counts.waiting, { queue: queueName, state: 'waiting' });
      observableResult.observe(counts.delayed, { queue: queueName, state: 'delayed' });
    });

    queueMetrics.activeJobs.addCallback(async (observableResult) => {
      observableResult.observe(counts.active, { queue: queueName });
    });

    // Alert on high failure rate
    if (counts.failed > 100) {
      console.error(`High failure rate in queue ${queueName}: ${counts.failed} failed jobs`);
    }
  }
}

// Run health check every 30 seconds
setInterval(monitorQueueHealth, 30000);
```

### 5. Unified Logging Pipeline
```typescript
// Structured logging with correlation IDs
import winston from 'winston';
import { trace } from '@opentelemetry/api';

const logger = winston.createLogger({
  format: winston.format.combine(
    winston.format.timestamp(),
    winston.format.errors({ stack: true }),
    winston.format.json(),
    winston.format.printf((info) => {
      // Add trace context
      const span = trace.getActiveSpan();
      if (span) {
        const context = span.spanContext();
        info.traceId = context.traceId;
        info.spanId = context.spanId;
      }
      return JSON.stringify(info);
    })
  ),
  transports: [
    new winston.transports.Console(),
    new winston.transports.Http({
      host: process.env.LOG_ENDPOINT_HOST,
      port: parseInt(process.env.LOG_ENDPOINT_PORT || '443'),
      path: '/v1/logs',
      ssl: true,
    }),
  ],
});

// Log enrichment middleware
function enrichLogs(req: Request, res: Response, next: NextFunction) {
  const requestId = req.headers['x-request-id'] || crypto.randomUUID();
  const userId = req.headers['x-user-id'] || 'anonymous';

  // Attach to request context
  req.context = {
    requestId,
    userId,
    startTime: Date.now(),
  };

  // Log request
  logger.info('Request received', {
    requestId,
    userId,
    method: req.method,
    path: req.path,
    ip: req.ip,
  });

  // Log response
  const originalSend = res.send;
  res.send = function(data) {
    const duration = Date.now() - req.context.startTime;

    logger.info('Request completed', {
      requestId,
      userId,
      statusCode: res.statusCode,
      duration,
    });

    return originalSend.call(this, data);
  };

  next();
}
```

### 6. Alerting Rules Configuration
```yaml
# Prometheus alerting rules
groups:
  - name: flrts_alerts
    interval: 30s
    rules:
      # Database alerts
      - alert: HighDatabaseConnections
        expr: pg_stat_database_numbackends{datname="flrts"} > 80
        for: 5m
        labels:
          severity: warning
          service: database
        annotations:
          summary: "High number of database connections"
          description: "Database {{ $labels.datname }} has {{ $value }} active connections"

      - alert: SlowQueries
        expr: rate(pg_stat_statements_mean_exec_time[5m]) > 1000
        for: 10m
        labels:
          severity: warning
          service: database
        annotations:
          summary: "Slow database queries detected"
          description: "Query execution time averaging {{ $value }}ms"

      # Queue alerts
      - alert: QueueBacklog
        expr: redis_queue_size{state="waiting"} > 1000
        for: 15m
        labels:
          severity: critical
          service: queue
        annotations:
          summary: "Large queue backlog"
          description: "Queue {{ $labels.queue }} has {{ $value }} waiting jobs"

      - alert: HighJobFailureRate
        expr: rate(redis_queue_jobs_failed_total[5m]) > 0.1
        for: 10m
        labels:
          severity: warning
          service: queue
        annotations:
          summary: "High job failure rate"
          description: "Queue {{ $labels.queue }} failing at {{ $value }} jobs/sec"

      # Edge Function alerts
      - alert: EdgeFunctionErrors
        expr: rate(http_requests_total{status=~"5.."}[5m]) > 0.05
        for: 5m
        labels:
          severity: critical
          service: edge-functions
        annotations:
          summary: "High error rate in Edge Functions"
          description: "Error rate: {{ $value }} requests/sec"

      - alert: EdgeFunctionLatency
        expr: histogram_quantile(0.95, rate(http_request_duration_ms_bucket[5m])) > 1000
        for: 10m
        labels:
          severity: warning
          service: edge-functions
        annotations:
          summary: "High Edge Function latency"
          description: "95th percentile latency: {{ $value }}ms"
```

### 7. Dashboard Configuration
```typescript
// Grafana dashboard as code
const dashboardConfig = {
  title: "FLRTS Monitoring Dashboard",
  panels: [
    {
      title: "Request Rate",
      type: "graph",
      targets: [{
        expr: 'rate(http_requests_total[5m])',
        legendFormat: '{{method}} {{path}}',
      }],
    },
    {
      title: "Error Rate",
      type: "graph",
      targets: [{
        expr: 'rate(http_requests_total{status=~"5.."}[5m])',
        legendFormat: '{{method}} {{path}}',
      }],
    },
    {
      title: "Database Connections",
      type: "graph",
      targets: [{
        expr: 'pg_stat_database_numbackends',
        legendFormat: '{{datname}}',
      }],
    },
    {
      title: "Queue Size",
      type: "graph",
      targets: [{
        expr: 'redis_queue_size',
        legendFormat: '{{queue}} - {{state}}',
      }],
    },
    {
      title: "P95 Latency",
      type: "graph",
      targets: [{
        expr: 'histogram_quantile(0.95, rate(http_request_duration_ms_bucket[5m]))',
        legendFormat: 'P95',
      }],
    },
  ],
};
```

## Implementation Checklist

### Phase 1: Core Infrastructure
- [ ] Deploy OTel Collector (Docker/Kubernetes)
- [ ] Configure OTLP exporters
- [ ] Set up Prometheus for metrics storage
- [ ] Deploy Grafana for visualization

### Phase 2: Application Instrumentation
- [ ] Instrument Edge Functions with OTel SDK
- [ ] Add PostgreSQL monitoring views
- [ ] Instrument n8n workflows
- [ ] Add Redis queue metrics

### Phase 3: Logging & Tracing
- [ ] Implement structured logging
- [ ] Add correlation IDs
- [ ] Configure distributed tracing
- [ ] Set up log aggregation

### Phase 4: Alerting & Dashboards
- [ ] Define alerting rules
- [ ] Create Grafana dashboards
- [ ] Set up PagerDuty/Slack integration
- [ ] Configure SLA monitoring

## Testing Requirements

```typescript
// Test observability pipeline
describe('Observability Pipeline', () => {
  it('should generate traces for Edge Function calls', async () => {
    const response = await fetch('/api/test');
    const traceId = response.headers.get('x-trace-id');

    expect(traceId).toBeDefined();

    // Verify trace in backend
    const trace = await getTraceFromBackend(traceId);
    expect(trace.spans.length).toBeGreaterThan(0);
  });

  it('should correlate logs with traces', async () => {
    const response = await fetch('/api/test');
    const traceId = response.headers.get('x-trace-id');

    // Query logs with trace ID
    const logs = await queryLogs({ traceId });
    expect(logs.length).toBeGreaterThan(0);
    expect(logs[0].traceId).toBe(traceId);
  });

  it('should trigger alerts on threshold breach', async () => {
    // Simulate high error rate
    for (let i = 0; i < 100; i++) {
      await fetch('/api/error');
    }

    // Wait for alert
    await sleep(60000); // 1 minute

    const alerts = await getActiveAlerts();
    expect(alerts).toContainEqual(
      expect.objectContaining({
        name: 'EdgeFunctionErrors',
        state: 'firing',
      })
    );
  });
});
```

## Performance Specifications
- Trace sampling: 10% in production, 100% in development
- Metric collection interval: 10 seconds
- Log retention: 30 days
- Trace retention: 7 days
- Dashboard refresh rate: 5 seconds
- Alert evaluation interval: 30 seconds

## Security Requirements
- Encrypt telemetry data in transit (TLS)
- Authenticate OTel collector endpoints
- Sanitize sensitive data from logs/traces
- Implement RBAC for dashboard access
- Rotate API keys monthly
- Audit access to monitoring systems

## Developer Resources

### Essential Documentation
```bash
# OpenTelemetry setup
mcp__ref__ref_search_documentation "OpenTelemetry TypeScript SDK Edge Functions"

# Supabase monitoring
mcp__ref__ref_search_documentation "Supabase observability OpenTelemetry 2025"

# PostgreSQL monitoring
mcp__ref__ref_search_documentation "PostgreSQL pg_stat_statements monitoring metrics"
```

## Acceptance Criteria
1. ✅ All services emit traces with correlation IDs
2. ✅ Key metrics visible in Grafana dashboards
3. ✅ Alerts fire within 5 minutes of threshold breach
4. ✅ Logs are searchable by trace ID
5. ✅ P95 latency tracked for all endpoints
6. ✅ Database slow queries are identified and logged

## Notes for Developers
- Use OTel semantic conventions for consistent naming
- Always propagate trace context across service boundaries
- Sample traces in production to control costs
- Use structured logging for better searchability
- Monitor the monitors - ensure observability stack is healthy
- Document custom metrics and their meanings
- Test alerting rules in staging before production