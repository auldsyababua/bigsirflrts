# Story 1.7: Monitoring & Observability

## Overview

Implement comprehensive monitoring and observability for the FLRTS stack using our verified technology stack: OpenTelemetry for Node.js, Supabase PostgreSQL monitoring, n8n webhook monitoring, and BullMQ Redis queue metrics.

## Context

- **Epic**: Epic 1 - Infrastructure & Core Systems
- **Priority**: P1 (Important but less urgent)
- **Dependencies**:
  - Story 1.1 (Supabase setup) - COMPLETED
  - Story 1.6 (Redis Queue) - COMPLETED
- **Blocks**: Production deployment readiness

## Technical Requirements

### Environment Configuration

```bash
# Required Environment Variables (verified from OpenTelemetry docs)
OTEL_EXPORTER_OTLP_ENDPOINT=http://localhost:4318
OTEL_EXPORTER_OTLP_PROTOCOL=http/protobuf
OTEL_SERVICE_NAME=flrts-production
OTEL_SERVICE_VERSION=1.0.0
OTEL_DEPLOYMENT_ENVIRONMENT=production
OTEL_RESOURCE_ATTRIBUTES=service.namespace=flrts,team=engineering
OTEL_TRACES_SAMPLER=parentbased_traceidratio
OTEL_TRACES_SAMPLER_ARG=0.1
```

### 1. OpenTelemetry Setup for Node.js Services

**Source**: [OpenTelemetry Node.js Official Documentation](https://opentelemetry.io/docs/languages/js/getting-started/nodejs/#instrumentation)

Create `instrumentation.ts` file (must run before application code):

```typescript
/*instrumentation.ts*/
import { NodeSDK } from '@opentelemetry/sdk-node';
import { getNodeAutoInstrumentations } from '@opentelemetry/auto-instrumentations-node';
import { OTLPTraceExporter } from '@opentelemetry/exporter-trace-otlp-proto';
import { OTLPMetricExporter } from '@opentelemetry/exporter-metrics-otlp-proto';
import { PeriodicExportingMetricReader } from '@opentelemetry/sdk-metrics';

const sdk = new NodeSDK({
  traceExporter: new OTLPTraceExporter({
    // Default: http://localhost:4318/v1/traces
    url: process.env.OTEL_EXPORTER_OTLP_ENDPOINT + '/v1/traces',
    headers: {
      'authorization': `Bearer ${process.env.OTEL_API_KEY || ''}`,
    },
  }),
  metricReader: new PeriodicExportingMetricReader({
    exporter: new OTLPMetricExporter({
      // Default: http://localhost:4318/v1/metrics
      url: process.env.OTEL_EXPORTER_OTLP_ENDPOINT + '/v1/metrics',
      headers: {
        'authorization': `Bearer ${process.env.OTEL_API_KEY || ''}`,
      },
    }),
  }),
  instrumentations: [getNodeAutoInstrumentations()],
});

sdk.start();
```

**Required Dependencies**:
```bash
npm install @opentelemetry/sdk-node \
  @opentelemetry/api \
  @opentelemetry/auto-instrumentations-node \
  @opentelemetry/sdk-metrics \
  @opentelemetry/sdk-trace-node \
  @opentelemetry/exporter-trace-otlp-proto \
  @opentelemetry/exporter-metrics-otlp-proto
```

**Application Startup** (Node.js v20+ with TypeScript):
```bash
node --import ./instrumentation.ts app.ts
```

### 2. Supabase Edge Functions Monitoring

**Source**: [Supabase Edge Functions Sentry Monitoring Guide](https://supabase.com/docs/guides/functions/examples/sentry-monitoring)

Note: Supabase Edge Functions run in Deno, not Node.js. OpenTelemetry support is limited.

**Alternative: Enhanced Error Tracking with Sentry**:

```typescript
// supabase/functions/monitoring-example/index.ts
import * as Sentry from 'https://deno.land/x/sentry/index.mjs';

Sentry.init({
  dsn: Deno.env.get('SENTRY_DSN'),
  defaultIntegrations: false, // Required for Edge Functions
  tracesSampleRate: 1.0,
  profilesSampleRate: 1.0,
});

// Set region and execution_id as custom tags
Sentry.setTag('region', Deno.env.get('SB_REGION'));
Sentry.setTag('execution_id', Deno.env.get('SB_EXECUTION_ID'));

Deno.serve(async (req) => {
  return Sentry.withScope(async (scope) => {
    const startTime = performance.now();

    try {
      scope.setTag('method', req.method);
      scope.setTag('url', req.url);

      const { data } = await req.json();

      // Business logic here
      const result = { message: `Processed: ${data}` };

      const duration = performance.now() - startTime;
      scope.setContext('performance', { duration_ms: duration });

      return new Response(JSON.stringify(result), {
        headers: { 'Content-Type': 'application/json' }
      });

    } catch (e) {
      Sentry.captureException(e);
      await Sentry.flush(2000);

      return new Response(JSON.stringify({ error: 'Internal error' }), {
        status: 500,
        headers: { 'Content-Type': 'application/json' },
      });
    }
  });
});
```

### 3. PostgreSQL Monitoring (Supabase)

**Source**: [PostgreSQL pg_stat_statements Documentation](https://learn.microsoft.com/en-us/azure/postgresql/flexible-server/how-to-high-cpu-utilization)

**Configuration** (via Supabase Dashboard > Settings > Database):
```sql
-- Enable pg_stat_statements (may already be enabled in Supabase)
-- shared_preload_libraries = 'pg_stat_statements'

-- Create monitoring views
CREATE SCHEMA IF NOT EXISTS monitoring;

-- Database performance metrics
CREATE OR REPLACE VIEW monitoring.database_metrics AS
SELECT
  current_database() as database_name,
  numbackends as active_connections,
  xact_commit as transactions_committed,
  xact_rollback as transactions_rolled_back,
  blks_read as blocks_read,
  blks_hit as blocks_hit,
  ROUND(100.0 * blks_hit / NULLIF(blks_hit + blks_read, 0), 2) as cache_hit_ratio,
  tup_returned as rows_returned,
  tup_fetched as rows_fetched,
  tup_inserted as rows_inserted,
  tup_updated as rows_updated,
  tup_deleted as rows_deleted,
  conflicts as replication_conflicts,
  deadlocks,
  NOW() - stats_reset as stats_collection_duration
FROM pg_stat_database
WHERE datname = current_database();

-- Slow query identification
CREATE OR REPLACE VIEW monitoring.slow_queries AS
SELECT
  queryid,
  LEFT(query, 100) as query_preview,
  calls,
  ROUND(total_exec_time::numeric, 2) as total_exec_time_ms,
  ROUND(mean_exec_time::numeric, 2) as mean_exec_time_ms,
  ROUND(max_exec_time::numeric, 2) as max_exec_time_ms,
  ROUND(stddev_exec_time::numeric, 2) as stddev_exec_time_ms,
  rows,
  ROUND(100.0 * shared_blks_hit / NULLIF(shared_blks_hit + shared_blks_read, 0), 2) AS cache_hit_ratio
FROM pg_stat_statements
WHERE mean_exec_time > 100  -- Queries averaging over 100ms
ORDER BY mean_exec_time DESC
LIMIT 20;

-- Connection monitoring
CREATE OR REPLACE VIEW monitoring.active_connections AS
SELECT
  count(*) as total_connections,
  count(*) FILTER (WHERE state = 'active') as active_connections,
  count(*) FILTER (WHERE state = 'idle') as idle_connections,
  count(*) FILTER (WHERE state = 'idle in transaction') as idle_in_transaction,
  count(*) FILTER (WHERE state = 'idle in transaction (aborted)') as idle_in_transaction_aborted
FROM pg_stat_activity
WHERE pid <> pg_backend_pid();
```

### 4. n8n Workflow Monitoring

**Source**: [n8n v1.105.2 Webhook Node Documentation](https://github.com/n8n-io/n8n)

**Built-in Webhook Monitoring** - Use n8n's webhook node with custom response handling:

```typescript
// Custom n8n webhook response for monitoring
// This is the response configuration in the Webhook node
{
  "responseMode": "responseNode",
  "responseData": {
    "status": 200,
    "headers": {
      "x-response-time": "{{Date.now() - $json.request_start_time}}",
      "x-workflow-id": "{{$workflow.id}}",
      "x-execution-id": "{{$execution.id}}"
    },
    "body": {
      "success": true,
      "processing_time_ms": "{{Date.now() - $json.request_start_time}}",
      "workflow": "{{$workflow.name}}",
      "items_processed": "{{$json.items.length}}"
    }
  }
}
```

**n8n Metrics Collection** via HTTP Request node to metrics endpoint:

```typescript
// HTTP Request node configuration for metrics collection
{
  "method": "POST",
  "url": "http://localhost:3001/metrics/n8n",
  "headers": {
    "Content-Type": "application/json"
  },
  "body": {
    "workflow_id": "{{$workflow.id}}",
    "workflow_name": "{{$workflow.name}}",
    "execution_id": "{{$execution.id}}",
    "timestamp": "{{Date.now()}}",
    "success": "{{$json.success || false}}",
    "duration_ms": "{{$json.duration_ms || 0}}",
    "items_count": "{{$json.items?.length || 0}}"
  }
}
```

### 5. Redis Queue Monitoring with BullMQ

**Source**: [BullMQ Prometheus Metrics Documentation](https://github.com/taskforcesh/bullmq/blob/master/docs/gitbook/guide/metrics/prometheus.md)

**Express.js Metrics Endpoint**:

```typescript
// metrics-server.ts
import express from 'express';
import { Queue } from 'bullmq';

const app = express();

// Initialize queues with Redis connection
const queues = {
  'flrts-tasks': new Queue('flrts-tasks', {
    connection: {
      host: process.env.REDIS_HOST || 'localhost',
      port: parseInt(process.env.REDIS_PORT || '6379'),
    }
  }),
  'flrts-reminders': new Queue('flrts-reminders', {
    connection: {
      host: process.env.REDIS_HOST || 'localhost',
      port: parseInt(process.env.REDIS_PORT || '6379'),
    }
  }),
};

// Prometheus metrics endpoint
app.get('/metrics', async (req, res) => {
  try {
    let allMetrics = '';

    for (const [queueName, queue] of Object.entries(queues)) {
      const globalVariables = {
        env: process.env.NODE_ENV || 'development',
        queue: queueName
      };

      const metrics = await queue.exportPrometheusMetrics(globalVariables);
      allMetrics += metrics + '\n';
    }

    res.set('Content-Type', 'text/plain');
    res.send(allMetrics);
  } catch (err) {
    console.error('Error exporting metrics:', err);
    res.status(500).send(`Error: ${err instanceof Error ? err.message : 'Unknown error'}`);
  }
});

// Health check endpoint
app.get('/health', async (req, res) => {
  try {
    const health = {};

    for (const [queueName, queue] of Object.entries(queues)) {
      const counts = await queue.getJobCounts();
      health[queueName] = {
        waiting: counts.waiting,
        active: counts.active,
        completed: counts.completed,
        failed: counts.failed,
        delayed: counts.delayed,
      };
    }

    res.json({ status: 'healthy', queues: health });
  } catch (err) {
    res.status(500).json({
      status: 'unhealthy',
      error: err instanceof Error ? err.message : 'Unknown error'
    });
  }
});

const PORT = process.env.METRICS_PORT || 3001;
app.listen(PORT, () => {
  console.log(`Metrics server running on port ${PORT}`);
  console.log(`Metrics: http://localhost:${PORT}/metrics`);
  console.log(`Health: http://localhost:${PORT}/health`);
});
```

**Required Dependencies**:
```bash
npm install bullmq express @types/express
```

### 6. Unified Logging with Winston

**Source**: Our tech stack uses Winston for logging

```typescript
// logger.ts
import winston from 'winston';
import { trace } from '@opentelemetry/api';

export const logger = winston.createLogger({
  level: process.env.LOG_LEVEL || 'info',
  format: winston.format.combine(
    winston.format.timestamp(),
    winston.format.errors({ stack: true }),
    winston.format.json(),
    winston.format.printf((info) => {
      // Add OpenTelemetry trace context
      const span = trace.getActiveSpan();
      if (span) {
        const context = span.spanContext();
        info.traceId = context.traceId;
        info.spanId = context.spanId;
      }
      return JSON.stringify(info);
    })
  ),
  defaultMeta: {
    service: process.env.OTEL_SERVICE_NAME || 'flrts',
    environment: process.env.NODE_ENV || 'development',
  },
  transports: [
    new winston.transports.Console(),
    new winston.transports.File({
      filename: 'logs/error.log',
      level: 'error'
    }),
    new winston.transports.File({
      filename: 'logs/combined.log'
    }),
  ],
});

// Express.js middleware for request logging
export function requestLogger(req: any, res: any, next: any) {
  const startTime = Date.now();
  const requestId = req.headers['x-request-id'] || crypto.randomUUID();

  req.requestId = requestId;

  logger.info('Request started', {
    requestId,
    method: req.method,
    url: req.url,
    userAgent: req.get('User-Agent'),
    ip: req.ip,
  });

  res.on('finish', () => {
    const duration = Date.now() - startTime;

    logger.info('Request completed', {
      requestId,
      method: req.method,
      url: req.url,
      statusCode: res.statusCode,
      duration,
    });
  });

  next();
}
```

## Docker Compose Infrastructure

```yaml
# docker-compose.monitoring.yml
version: '3.8'

services:
  # Prometheus for metrics collection
  prometheus:
    image: prom/prometheus:v2.45.0
    container_name: flrts-prometheus
    ports:
      - "9090:9090"
    volumes:
      - ./monitoring/prometheus.yml:/etc/prometheus/prometheus.yml
      - prometheus-data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'

  # Grafana for visualization
  grafana:
    image: grafana/grafana:10.0.0
    container_name: flrts-grafana
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=admin
    volumes:
      - grafana-data:/var/lib/grafana
      - ./monitoring/grafana:/etc/grafana/provisioning

  # Jaeger for distributed tracing
  jaeger:
    image: jaegertracing/all-in-one:1.49
    container_name: flrts-jaeger
    ports:
      - "16686:16686"  # Jaeger UI
      - "14250:14250"  # gRPC
      - "4317:4317"    # OTLP gRPC
      - "4318:4318"    # OTLP HTTP
    environment:
      - COLLECTOR_OTLP_ENABLED=true

volumes:
  prometheus-data:
  grafana-data:
```

## Implementation Checklist

### Phase 1: Foundation (Week 1)
- [ ] Install OpenTelemetry packages for Node.js services
- [ ] Create instrumentation.ts configuration file
- [ ] Deploy monitoring infrastructure (Prometheus, Grafana, Jaeger)
- [ ] Test basic tracing with simple Express.js endpoint

### Phase 2: Application Instrumentation (Week 2)
- [ ] Instrument main FLRTS services with OpenTelemetry
- [ ] Create PostgreSQL monitoring views in Supabase
- [ ] Set up BullMQ Prometheus metrics endpoint
- [ ] Configure Sentry for Supabase Edge Functions

### Phase 3: Monitoring & Alerting (Week 3)
- [ ] Create Grafana dashboards for key metrics
- [ ] Configure basic alerting rules
- [ ] Set up log aggregation and correlation
- [ ] Test end-to-end observability pipeline

### Phase 4: Production Readiness (Week 4)
- [ ] Optimize trace sampling for production
- [ ] Configure log retention policies
- [ ] Set up automated backups for monitoring data
- [ ] Document runbooks and troubleshooting guides

## Acceptance Criteria

1. ✅ **Distributed Tracing**: All Node.js services emit OpenTelemetry traces
2. ✅ **Database Monitoring**: PostgreSQL performance metrics visible in Grafana
3. ✅ **Queue Monitoring**: BullMQ metrics accessible via /metrics endpoint
4. ✅ **Error Tracking**: Edge Functions report errors to Sentry
5. ✅ **Log Correlation**: Logs include trace IDs for correlation
6. ✅ **Alerting**: Basic alerts configured for high error rates and slow queries
7. ✅ **Documentation**: Setup and troubleshooting guides completed

## Performance Targets

| Metric | Target | Monitoring |
|--------|--------|------------|
| API Response Time | <200ms p95 | OpenTelemetry HTTP instrumentation |
| Database Query Time | <50ms p95 | pg_stat_statements |
| Queue Processing | <5s p95 | BullMQ metrics |
| Error Rate | <1% | OpenTelemetry + Sentry |
| Trace Collection Overhead | <5% CPU | OpenTelemetry sampling |

## Security Considerations

- Sanitize sensitive data from logs and traces
- Use API keys for external monitoring services
- Implement RBAC for Grafana dashboard access
- Encrypt telemetry data in transit with TLS
- Regular security updates for monitoring stack components

## Dev Notes - Research Sources

**OpenTelemetry Setup**: [Official Node.js Documentation](https://opentelemetry.io/docs/languages/js/getting-started/nodejs/#instrumentation)
- Verified instrumentation.ts pattern
- Confirmed OTLP exporter configuration
- Validated environment variable names

**Supabase Monitoring**: [Sentry Integration Guide](https://supabase.com/docs/guides/functions/examples/sentry-monitoring)
- Edge Functions use Deno runtime (not Node.js)
- Sentry provides better error tracking than OpenTelemetry for Edge Functions
- withScope pattern required for proper error isolation

**PostgreSQL Monitoring**: [Azure PostgreSQL Performance Guide](https://learn.microsoft.com/en-us/azure/postgresql/flexible-server/how-to-high-cpu-utilization)
- pg_stat_statements configuration
- Standard monitoring views pattern
- Performance query examples

**BullMQ Metrics**: [Official Prometheus Documentation](https://github.com/taskforcesh/bullmq/blob/master/docs/gitbook/guide/metrics/prometheus.md)
- Native exportPrometheusMetrics() API
- Global variables for environment labeling
- Express.js integration pattern

**Tech Stack Verification**: /docs/architecture/tech-stack.md
- Confirmed Winston for logging
- Verified Express.js as web framework
- Validated Redis 7 + BullMQ for queues
- Confirmed Node.js 22 LTS + TypeScript 5.6
