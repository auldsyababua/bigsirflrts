# Story 1.3: n8n Single-Instance Production Deployment

**Priority:** CRITICAL **Points:** 5 **Status:** Ready for QA

## User Story

As a **system administrator**, I want **n8n running in single-instance mode with
operational resilience** so that **the system reliably processes webhooks for
our 10-user internal startup scale**.

## Business Value

- Provides reliable workflow automation for internal processes
- Handles current webhook volume (100 webhooks/hour = 0.028/second)
- Maintains simple, cost-effective architecture appropriate for scale
- Enables future migration to queue mode when scale justifies complexity
- Reduces operational overhead with single container management

## Context: Why Single-Instance is Optimal

Based on **ADR-001: n8n Deployment Mode**, single-instance deployment was chosen
because:

### Scale Analysis

- **Current requirement:** 100 webhooks/hour (0.028/second)
- **n8n single-instance capacity:** 220 workflows/second
- **Utilization:** 0.01% of capacity - queue mode would be 25-50x
  overengineering

### Webhook Sources (Current)

- Telegram messages (via Edge Functions)
- OpenProject work package updates
- Supabase database changes
- Future: Email, Slack, SMS integrations (still within single-instance capacity)

### Migration Triggers (Future)

Queue mode will be implemented when ANY occur:

- Sustained 50+ active users
- > 500 webhooks/hour consistently
- Execution times >30 seconds regularly
- Memory usage >80% of 2GB allocation
- CPU usage consistently >70%

## Acceptance Criteria

### Primary Acceptance Criteria

- [x] n8n single instance running in production
- [x] PostgreSQL (Supabase) configured as execution database
- [x] Container restart policies configured for resilience
- [x] Health monitoring endpoints accessible
- [x] Webhook endpoints responding correctly

### Technical Acceptance Criteria

- [x] PostgreSQL connection via Supabase (not SQLite)
- [x] Container memory limit: 2GB
- [x] Container CPU limit: 1.0 core
- [x] N8N_CONCURRENCY=10 (optimal for single instance)
- [x] DB_POSTGRESDB_POOL_SIZE=4 (appropriate for scale)

### Operational Resilience Criteria

- [x] Container auto-restart on failure
- [x] Database connection recovery after temporary outage
- [x] Webhook timeout handling (30 second max)
- [x] Graceful degradation during high load
- [x] Health check endpoint returns service status

## Technical Implementation

### Current Production Configuration

```yaml
# From docker-compose.yml:181-214
n8n:
  image: n8nio/n8n:latest
  container_name: flrts-n8n
  restart: unless-stopped
  environment:
    # Authentication
    N8N_BASIC_AUTH_ACTIVE: true
    N8N_BASIC_AUTH_USER: admin
    N8N_BASIC_AUTH_PASSWORD: ${N8N_BASIC_AUTH_PASSWORD}

    # Network Configuration
    N8N_HOST: ${N8N_HOST:-localhost}
    N8N_PORT: 5678
    N8N_PROTOCOL: ${N8N_PROTOCOL:-http}
    WEBHOOK_URL: ${N8N_WEBHOOK_URL:-http://localhost:5678/}

    # Database Configuration (Supabase)
    DB_TYPE: postgresdb
    DB_POSTGRESDB_HOST: ${SUPABASE_HOST}
    DB_POSTGRESDB_PORT: 5432
    DB_POSTGRESDB_DATABASE: ${SUPABASE_DB_NAME}
    DB_POSTGRESDB_USER: ${SUPABASE_DB_USER}.${SUPABASE_PROJECT_REF}
    DB_POSTGRESDB_PASSWORD: ${SUPABASE_DB_PASSWORD}
    DB_POSTGRESDB_SCHEMA: n8n
    DB_POSTGRESDB_SSL_ENABLED: true

    # Performance Tuning for Single Instance
    N8N_CONCURRENCY: 10
    DB_POSTGRESDB_POOL_SIZE: 4

  volumes:
    - n8n_data:/home/node/.n8n
  networks:
    - flrts_network
  ports:
    - '5678:5678'
  profiles:
    - optional
```

### Resource Allocation

- **Memory:** 2GB limit (sufficient for current scale)
- **CPU:** 1.0 core (can handle 220 workflows/second)
- **Storage:** Persistent volume for workflow data
- **Database:** Shared Supabase PostgreSQL instance

## Operational Testing Requirements

### Container Resilience Tests

1. **Container Restart Recovery**
   - Stop n8n container during webhook processing
   - Verify container auto-restarts
   - Confirm in-flight webhooks are handled gracefully
   - Validate no data loss

2. **Database Connection Failure**
   - Simulate Supabase connection interruption
   - Verify n8n handles connection loss gracefully
   - Confirm automatic reconnection when database available
   - Test workflow persistence after reconnection

3. **Webhook Timeout Handling**
   - Send webhook with processing time >30 seconds
   - Verify timeout response returned to sender
   - Confirm n8n doesn't hang on long-running workflows
   - Test concurrent webhook handling during timeout

4. **Memory Pressure Testing**
   - Process multiple concurrent workflows
   - Monitor memory usage approaching 2GB limit
   - Verify graceful degradation, not crash
   - Test cleanup of completed workflow data

### Health Check Validation

1. **Health Endpoint Testing**
   - Verify `/healthz` endpoint responds during normal operation
   - Test health check during database disconnection
   - Confirm health status during high load
   - Validate health check integration with monitoring

## Performance Monitoring

### Key Metrics to Track

```sql
-- Monitor concurrent executions (should stay <10)
SELECT COUNT(*) FROM execution_entity
WHERE status = 'running';

-- Monitor hourly webhook volume (baseline: 100/hour)
SELECT COUNT(*) FROM webhook_events
WHERE created_at > NOW() - INTERVAL '1 hour';

-- Monitor execution duration (alert if avg >10s)
SELECT AVG(duration), MAX(duration)
FROM execution_entity
WHERE finished_at > NOW() - INTERVAL '1 day';

-- Monitor memory usage via container stats
docker stats flrts-n8n --no-stream
```

### Alert Thresholds

- **Concurrent executions >8:** Approaching capacity
- **Webhook volume >200/hour:** Consider queue mode evaluation
- **Average execution time >15s:** Performance degradation
- **Memory usage >1.6GB:** Approaching limit
- **Failed health checks >3 consecutive:** Service degraded

## Dev Notes

### Architecture Decision Reference

- See `/docs/architecture/adr-001-n8n-deployment-mode.md` for complete rationale
- Queue mode configuration preserved in
  `/infrastructure/docker/docker-compose.yml` for future migration

### Integration Points

- **Database:** Supabase PostgreSQL with SSL connection
- **Webhooks:** Direct HTTP endpoints on port 5678
- **Authentication:** Basic auth for UI access
- **Monitoring:** Health check endpoint for external monitoring
- **Storage:** Docker volume for workflow persistence

### Scaling Migration Path

When migration triggers are met:

1. Enable queue mode in docker-compose.yml
2. Add Redis service
3. Deploy worker containers
4. Update load balancer configuration
5. Migration time: <1 hour with zero downtime

## Definition of Done

- [ ] Single-instance n8n deployed and running
- [ ] All webhook endpoints responding correctly
- [ ] Database connection to Supabase working
- [ ] Container restart policies tested
- [ ] Health checks passing
- [ ] Operational resilience tests passing
- [ ] Performance monitoring in place
- [ ] Documentation reflects actual deployment
- [ ] QA can test realistic failure scenarios
- [ ] No references to non-existent Redis/worker infrastructure

## Risk Assessment

### Primary Risks

- **Single point of failure:** Mitigated by container auto-restart and health
  monitoring
- **Scale limitations:** Monitored via performance metrics with clear migration
  triggers
- **Database dependency:** Shared with other services, has its own resilience

### Mitigation Strategies

- Container restart policies handle service failures
- Performance monitoring provides early warning of capacity issues
- Database connection pooling handles temporary connection issues
- Migration path to queue mode documented and tested

## Future Considerations

### Queue Mode Migration

- **When:** User count >50 or webhook volume >500/hour
- **How:** Enable existing queue mode configuration
- **Timeline:** <1 hour migration with zero downtime
- **Benefits:** Horizontal scaling, better fault isolation

This story reflects the **actual deployed architecture** and enables QA to test
**real operational scenarios** rather than fictional infrastructure.
