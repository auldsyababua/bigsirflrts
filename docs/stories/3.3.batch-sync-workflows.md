# Story 3.3: Batch Sync Workflows

## Overview
Implement efficient batch synchronization between FLRTS and OpenProject, handling bulk updates, conflict resolution, and maintaining data consistency across systems.

## Context
- **Epic**: Epic 3 - OpenProject Integration
- **Priority**: P1 (Important for scale)
- **Dependencies**:
  - Story 1.1 (Supabase project setup) - COMPLETED
  - Story 1.6 (Redis queue configuration) - Required for queue management
  - Story 3.1 (OpenProject API workflows) - Required for API operations
- **Blocks**: Story 4.1, 4.2 (List features depend on reliable sync)

## Technical Requirements

### Batch Processing Architecture
```typescript
// Batch sync configuration
interface BatchSyncConfig {
  BATCH_SIZE: number;           // Default: 50 items
  PARALLEL_WORKERS: number;     // Default: 3
  SYNC_INTERVAL_MINUTES: number; // Default: 15
  MAX_RETRIES: number;          // Default: 3
  CONFLICT_STRATEGY: 'server_wins' | 'client_wins' | 'newest_wins';
  STAGING_TABLE_PREFIX: string; // 'sync_staging_'
}

// Sync operation types
enum SyncOperation {
  FULL_SYNC = 'full_sync',      // Complete data refresh
  INCREMENTAL = 'incremental',   // Changed items only
  DELTA_SYNC = 'delta_sync',    // Bi-directional changes
  RECONCILIATION = 'reconciliation' // Conflict resolution
}
```

### PostgreSQL Staging Table Pattern
```sql
-- Create staging table for batch operations
CREATE TEMP TABLE sync_staging_work_packages (
  id SERIAL PRIMARY KEY,
  external_id VARCHAR(255) UNIQUE,
  operation VARCHAR(20) NOT NULL, -- 'insert', 'update', 'delete'
  data JSONB NOT NULL,
  processed_at TIMESTAMP,
  error_message TEXT,
  retry_count INTEGER DEFAULT 0,
  checksum VARCHAR(64),
  created_at TIMESTAMP DEFAULT NOW()
) ON COMMIT DROP;

-- Index for efficient processing
CREATE INDEX idx_staging_operation ON sync_staging_work_packages(operation, processed_at);
CREATE INDEX idx_staging_external_id ON sync_staging_work_packages(external_id);
```

### n8n Batch Sync Workflow
```json
{
  "name": "Batch Sync Manager",
  "nodes": [
    {
      "type": "n8n-nodes-base.schedule",
      "name": "Sync Schedule",
      "parameters": {
        "rule": {
          "interval": [{ "field": "minutes", "value": 15 }]
        }
      }
    },
    {
      "type": "n8n-nodes-base.postgres",
      "name": "Get Changed Items",
      "parameters": {
        "operation": "executeQuery",
        "query": `
          SELECT * FROM tasks
          WHERE updated_at > $1
          ORDER BY updated_at ASC
          LIMIT $2
        `,
        "additionalFields": {
          "queryParams": "={{$json.lastSyncTime}},{{$json.batchSize}}"
        }
      }
    },
    {
      "type": "n8n-nodes-base.splitInBatches",
      "name": "Batch Processor",
      "parameters": {
        "batchSize": 50,
        "options": {
          "reset": false
        }
      }
    },
    {
      "type": "n8n-nodes-base.postgres",
      "name": "Stage Batch",
      "parameters": {
        "operation": "insert",
        "table": "sync_staging_work_packages",
        "columns": "external_id,operation,data,checksum",
        "returnFields": "*"
      }
    },
    {
      "type": "n8n-nodes-base.function",
      "name": "Process Batch",
      "parameters": {
        "functionCode": `
          // Bulk upsert using staging table
          const batchResult = await $db.query(\`
            WITH staged AS (
              SELECT * FROM sync_staging_work_packages
              WHERE processed_at IS NULL
              LIMIT 50
            ),
            upserted AS (
              INSERT INTO work_packages (external_id, data, checksum)
              SELECT external_id, data, checksum
              FROM staged
              ON CONFLICT (external_id)
              DO UPDATE SET
                data = EXCLUDED.data,
                checksum = EXCLUDED.checksum,
                updated_at = NOW()
              WHERE work_packages.checksum != EXCLUDED.checksum
              RETURNING external_id
            )
            UPDATE sync_staging_work_packages s
            SET processed_at = NOW()
            FROM upserted u
            WHERE s.external_id = u.external_id
            RETURNING u.external_id;
          \`);

          return { processed: batchResult.rows.length };
        `
      }
    }
  ]
}
```

### Efficient Bulk Operations

#### 1. PostgreSQL COPY for Large Datasets
```typescript
import { Readable } from 'stream';
import { Pool } from 'pg';
import { stringify } from 'csv-stringify';

class BulkDataLoader {
  private pool: Pool;

  constructor(connectionString: string) {
    this.pool = new Pool({ connectionString });
  }

  async bulkLoad(
    tableName: string,
    data: any[],
    columns: string[]
  ): Promise<number> {
    const client = await this.pool.connect();

    try {
      await client.query('BEGIN');

      // Create staging table
      await client.query(`
        CREATE TEMP TABLE ${tableName}_staging
        (LIKE ${tableName} INCLUDING ALL)
        ON COMMIT DROP
      `);

      // Stream data using COPY
      const stream = Readable.from(
        stringify(data, {
          header: false,
          columns: columns
        })
      );

      const copyQuery = `
        COPY ${tableName}_staging (${columns.join(',')})
        FROM STDIN WITH (FORMAT csv, HEADER false)
      `;

      await new Promise((resolve, reject) => {
        const copyStream = client.query(copyFrom(copyQuery));
        stream.pipe(copyStream)
          .on('error', reject)
          .on('finish', resolve);
      });

      // Merge from staging to main table
      const mergeResult = await client.query(`
        INSERT INTO ${tableName} (${columns.join(',')})
        SELECT ${columns.join(',')}
        FROM ${tableName}_staging
        ON CONFLICT (id) DO UPDATE SET
          ${columns.map(col => `${col} = EXCLUDED.${col}`).join(',')},
          updated_at = NOW()
        RETURNING id
      `);

      await client.query('COMMIT');
      return mergeResult.rowCount;

    } catch (error) {
      await client.query('ROLLBACK');
      throw error;
    } finally {
      client.release();
    }
  }
}
```

#### 2. Parallel Processing with BullMQ
```typescript
import { Queue, Worker, Job, FlowProducer } from 'bullmq';

interface SyncBatch {
  batchId: string;
  startId: number;
  endId: number;
  operation: SyncOperation;
}

class BatchSyncOrchestrator {
  private flowProducer: FlowProducer;
  private connection: IORedis;

  constructor(redisConfig: RedisConfig) {
    this.connection = new IORedis(redisConfig);
    this.flowProducer = new FlowProducer({ connection: this.connection });
  }

  async orchestrateBatchSync(
    totalItems: number,
    batchSize: number = 50
  ): Promise<string> {
    const batches: any[] = [];
    const batchCount = Math.ceil(totalItems / batchSize);

    // Create batch jobs
    for (let i = 0; i < batchCount; i++) {
      batches.push({
        name: 'process-batch',
        queueName: 'sync-queue',
        data: {
          batchId: `batch-${i}`,
          startId: i * batchSize,
          endId: Math.min((i + 1) * batchSize, totalItems),
          operation: SyncOperation.INCREMENTAL
        },
        opts: {
          attempts: 3,
          backoff: { type: 'exponential', delay: 2000 }
        }
      });
    }

    // Create flow with parallel batch processing
    const flow = await this.flowProducer.add({
      name: 'batch-sync-flow',
      queueName: 'orchestrator',
      children: batches,
      opts: {
        failParentOnFailure: false
      }
    });

    return flow.job.id;
  }
}

// Batch processor worker
const batchWorker = new Worker<SyncBatch>(
  'sync-queue',
  async (job: Job<SyncBatch>) => {
    const { batchId, startId, endId, operation } = job.data;

    // Process batch with progress reporting
    const total = endId - startId;
    let processed = 0;

    for (let id = startId; id < endId; id++) {
      // Process individual item
      await processItem(id, operation);

      processed++;
      // Report progress
      await job.updateProgress((processed / total) * 100);
    }

    return { batchId, processed, success: true };
  },
  {
    concurrency: 3, // Process 3 batches in parallel
    limiter: {
      max: 10,
      duration: 60000 // 10 batches per minute
    }
  }
);
```

### Conflict Resolution Strategies

#### 1. Version-Based Conflict Detection
```typescript
interface VersionedEntity {
  id: string;
  version: number;
  checksum: string;
  localUpdatedAt: Date;
  remoteUpdatedAt: Date;
  data: any;
}

class ConflictResolver {
  async detectConflicts(
    local: VersionedEntity[],
    remote: VersionedEntity[]
  ): Promise<ConflictReport> {
    const conflicts: Conflict[] = [];
    const remoteMap = new Map(remote.map(r => [r.id, r]));

    for (const localItem of local) {
      const remoteItem = remoteMap.get(localItem.id);

      if (!remoteItem) continue;

      // Check for conflicts
      if (localItem.checksum !== remoteItem.checksum) {
        if (localItem.version === remoteItem.version) {
          // Concurrent modification detected
          conflicts.push({
            type: 'concurrent_modification',
            localItem,
            remoteItem,
            resolution: this.resolveConflict(localItem, remoteItem)
          });
        } else if (localItem.version < remoteItem.version) {
          // Local is behind
          conflicts.push({
            type: 'local_behind',
            localItem,
            remoteItem,
            resolution: 'update_local'
          });
        }
      }
    }

    return { conflicts, resolutionStrategy: this.strategy };
  }

  private resolveConflict(
    local: VersionedEntity,
    remote: VersionedEntity
  ): string {
    switch (this.strategy) {
      case 'newest_wins':
        return local.localUpdatedAt > remote.remoteUpdatedAt
          ? 'keep_local'
          : 'use_remote';

      case 'server_wins':
        return 'use_remote';

      case 'client_wins':
        return 'keep_local';

      default:
        return 'manual_resolution';
    }
  }
}
```

#### 2. Three-Way Merge Pattern
```typescript
interface MergeableEntity {
  id: string;
  baseVersion: any;  // Common ancestor
  localVersion: any; // Local changes
  remoteVersion: any; // Remote changes
}

class ThreeWayMerger {
  async merge(entity: MergeableEntity): Promise<MergeResult> {
    const { baseVersion, localVersion, remoteVersion } = entity;

    // Detect changes
    const localChanges = this.diff(baseVersion, localVersion);
    const remoteChanges = this.diff(baseVersion, remoteVersion);

    // Check for conflicts
    const conflicts = this.findConflicts(localChanges, remoteChanges);

    if (conflicts.length === 0) {
      // Auto-merge non-conflicting changes
      return {
        success: true,
        merged: this.applyChanges(baseVersion, [...localChanges, ...remoteChanges]),
        conflicts: []
      };
    }

    // Handle conflicts
    return {
      success: false,
      merged: null,
      conflicts: conflicts.map(c => ({
        field: c.field,
        localValue: c.localValue,
        remoteValue: c.remoteValue,
        suggestedResolution: this.suggestResolution(c)
      }))
    };
  }

  private diff(base: any, modified: any): Change[] {
    const changes: Change[] = [];

    for (const key in modified) {
      if (base[key] !== modified[key]) {
        changes.push({
          field: key,
          oldValue: base[key],
          newValue: modified[key],
          operation: base[key] === undefined ? 'add' : 'modify'
        });
      }
    }

    return changes;
  }
}
```

### Monitoring & Observability

#### 1. Sync Metrics Collection
```typescript
interface SyncMetrics {
  totalItems: number;
  processedItems: number;
  failedItems: number;
  conflictsDetected: number;
  conflictsResolved: number;
  syncDuration: number;
  averageItemTime: number;
}

class SyncMonitor {
  private metrics: Map<string, SyncMetrics> = new Map();

  async recordSyncRun(
    syncId: string,
    metrics: Partial<SyncMetrics>
  ): Promise<void> {
    const existing = this.metrics.get(syncId) || this.createEmptyMetrics();
    const updated = { ...existing, ...metrics };

    this.metrics.set(syncId, updated);

    // Store in database for historical tracking
    await this.persistMetrics(syncId, updated);

    // Send to monitoring service
    await this.sendToMonitoring(syncId, updated);
  }

  async generateSyncReport(
    startDate: Date,
    endDate: Date
  ): Promise<SyncReport> {
    const query = `
      SELECT
        DATE_TRUNC('hour', created_at) as hour,
        COUNT(*) as sync_runs,
        AVG(total_items) as avg_items,
        AVG(sync_duration) as avg_duration,
        SUM(failed_items) as total_failures,
        SUM(conflicts_detected) as total_conflicts
      FROM sync_metrics
      WHERE created_at BETWEEN $1 AND $2
      GROUP BY hour
      ORDER BY hour DESC
    `;

    const results = await db.query(query, [startDate, endDate]);

    return {
      period: { startDate, endDate },
      summary: this.calculateSummary(results.rows),
      hourlyBreakdown: results.rows
    };
  }
}
```

#### 2. Health Checks
```typescript
class SyncHealthChecker {
  async checkSyncHealth(): Promise<HealthStatus> {
    const checks = await Promise.all([
      this.checkLastSyncTime(),
      this.checkSyncQueue(),
      this.checkErrorRate(),
      this.checkDataConsistency()
    ]);

    const overallHealth = checks.every(c => c.healthy)
      ? 'healthy'
      : checks.some(c => !c.healthy && c.severity === 'critical')
        ? 'critical'
        : 'degraded';

    return {
      status: overallHealth,
      checks,
      timestamp: new Date()
    };
  }

  private async checkDataConsistency(): Promise<HealthCheck> {
    const query = `
      WITH consistency_check AS (
        SELECT
          COUNT(DISTINCT l.id) as local_count,
          COUNT(DISTINCT r.external_id) as remote_count,
          COUNT(DISTINCT l.id) - COUNT(DISTINCT r.external_id) as diff
        FROM local_tasks l
        FULL OUTER JOIN remote_tasks r ON l.id = r.external_id
      )
      SELECT * FROM consistency_check
    `;

    const result = await db.query(query);
    const diff = Math.abs(result.rows[0].diff);

    return {
      name: 'data_consistency',
      healthy: diff < 10, // Tolerate small differences
      message: `${diff} items out of sync`,
      severity: diff > 100 ? 'critical' : 'warning'
    };
  }
}
```

## Implementation Checklist

### Phase 1: Basic Batch Processing
- [ ] Set up staging tables for batch operations
- [ ] Implement PostgreSQL COPY for bulk inserts
- [ ] Create n8n batch sync workflow
- [ ] Add basic conflict detection

### Phase 2: Advanced Sync Features
- [ ] Implement parallel batch processing with BullMQ
- [ ] Add three-way merge for conflict resolution
- [ ] Create incremental sync with change tracking
- [ ] Implement checksum-based change detection

### Phase 3: Monitoring & Recovery
- [ ] Add sync metrics collection
- [ ] Implement health checks and alerts
- [ ] Create sync status dashboard
- [ ] Add automatic retry and recovery

### Phase 4: Optimization
- [ ] Optimize batch sizes based on performance
- [ ] Implement adaptive sync intervals
- [ ] Add caching for frequently accessed data
- [ ] Create sync performance reports

## Testing Requirements

```typescript
describe('Batch Sync Operations', () => {
  let syncManager: BatchSyncOrchestrator;
  let mockData: any[];

  beforeEach(() => {
    syncManager = new BatchSyncOrchestrator(testRedisConfig);
    mockData = generateMockTasks(1000); // Generate 1000 test items
  });

  it('should process large batches efficiently', async () => {
    const startTime = Date.now();
    const result = await syncManager.orchestrateBatchSync(1000, 50);
    const duration = Date.now() - startTime;

    expect(result).toBeDefined();
    expect(duration).toBeLessThan(10000); // Complete within 10 seconds
  });

  it('should handle conflicts correctly', async () => {
    const local = [{ id: '1', version: 1, checksum: 'abc' }];
    const remote = [{ id: '1', version: 1, checksum: 'def' }];

    const resolver = new ConflictResolver('newest_wins');
    const conflicts = await resolver.detectConflicts(local, remote);

    expect(conflicts.conflicts).toHaveLength(1);
    expect(conflicts.conflicts[0].type).toBe('concurrent_modification');
  });

  it('should maintain data consistency during sync', async () => {
    // Create test data with known checksums
    const testData = createTestDataWithChecksums(100);

    // Run sync
    await syncManager.syncBatch(testData);

    // Verify checksums match
    const verified = await verifyDataIntegrity();
    expect(verified.corrupted).toBe(0);
  });
});
```

## Performance Specifications
- Batch processing: 10,000+ items per minute
- Parallel workers: 3-10 concurrent batches
- Memory usage: < 500MB for 100,000 items
- Conflict resolution: < 100ms per conflict
- Sync latency: < 5 minutes for full sync

## Security Requirements
- Validate all data before batch operations
- Use prepared statements to prevent SQL injection
- Implement rate limiting on sync endpoints
- Encrypt sensitive data in staging tables
- Audit all sync operations with user attribution

## Developer Resources

### Essential Documentation
```bash
# PostgreSQL batch operations
mcp__ref__ref_search_documentation "PostgreSQL COPY bulk insert performance 2025"

# n8n batch processing
mcp__ref__ref_search_documentation "n8n splitInBatches node parallel processing"

# Conflict resolution patterns
mcp__ref__ref_search_documentation "three-way merge conflict resolution distributed systems"
```

### Debugging Commands
```bash
# Monitor staging table
psql -c "SELECT operation, COUNT(*), AVG(retry_count)
         FROM sync_staging_work_packages
         GROUP BY operation"

# Check sync queue status
redis-cli LLEN bull:sync-queue:wait

# View sync metrics
curl http://localhost:3000/api/sync/metrics

# Force sync run
n8n execute --id=<workflow-id> --data='{"operation":"full_sync"}'
```

## Acceptance Criteria
1. ✅ Batch sync processes 10,000+ items without timeout
2. ✅ Conflicts are detected and resolved automatically
3. ✅ Staging tables prevent data corruption
4. ✅ Parallel processing improves throughput 3x
5. ✅ Failed batches retry with exponential backoff
6. ✅ Sync metrics are collected and accessible

## Notes for Developers
- Always use staging tables for bulk operations to prevent locks
- Implement checksums for efficient change detection
- Use COPY instead of INSERT for datasets > 1000 rows
- Monitor memory usage during large syncs
- Test conflict resolution with production-like data
- Consider time zones when comparing timestamps