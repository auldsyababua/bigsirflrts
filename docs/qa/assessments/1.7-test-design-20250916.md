# Test Design: Story 1.7 - Monitoring & Observability

Date: 2025-09-16 Designer: Quinn (Test Architect)

## Test Strategy Overview

- Total test scenarios: 24
- Unit tests: 8 (33%)
- Integration tests: 10 (42%)
- E2E tests: 6 (25%)
- Priority distribution: P0: 12, P1: 8, P2: 4

## Test Scenarios by Acceptance Criteria

### AC1: Distributed Tracing - All Node.js services emit OpenTelemetry traces

#### Scenarios

| ID           | Level       | Priority | Test                                 | Justification                        |
| ------------ | ----------- | -------- | ------------------------------------ | ------------------------------------ |
| 1.7-UNIT-001 | Unit        | P0       | OpenTelemetry SDK initialization     | Critical configuration validation    |
| 1.7-UNIT-002 | Unit        | P0       | Trace exporter configuration         | Core tracing functionality           |
| 1.7-UNIT-003 | Unit        | P1       | Environment variable validation      | Configuration error prevention       |
| 1.7-INT-001  | Integration | P0       | Service-to-service trace propagation | Distributed tracing core requirement |
| 1.7-INT-002  | Integration | P0       | OTLP HTTP endpoint connectivity      | External dependency validation       |
| 1.7-E2E-001  | E2E         | P0       | End-to-end request trace collection  | Complete tracing pipeline validation |

### AC2: Database Monitoring - PostgreSQL performance metrics visible in Grafana

#### Scenarios

| ID           | Level       | Priority | Test                                  | Justification                        |
| ------------ | ----------- | -------- | ------------------------------------- | ------------------------------------ |
| 1.7-UNIT-004 | Unit        | P1       | SQL monitoring view syntax validation | Query correctness verification       |
| 1.7-INT-003  | Integration | P0       | pg_stat_statements data collection    | Database monitoring foundation       |
| 1.7-INT-004  | Integration | P0       | Monitoring views data accuracy        | Data integrity for dashboard metrics |
| 1.7-INT-005  | Integration | P1       | Slow query detection thresholds       | Performance monitoring capability    |
| 1.7-E2E-002  | E2E         | P1       | Grafana dashboard metric display      | User-facing monitoring interface     |

### AC3: n8n Monitoring - Webhook responses tracked with performance metrics

#### Scenarios

| ID           | Level       | Priority | Test                                 | Justification                     |
| ------------ | ----------- | -------- | ------------------------------------ | --------------------------------- |
| 1.7-UNIT-005 | Unit        | P1       | Webhook response header generation   | Performance metric injection      |
| 1.7-INT-006  | Integration | P0       | n8n workflow execution tracking      | Workflow monitoring capability    |
| 1.7-INT-007  | Integration | P1       | Metrics endpoint HTTP request config | External metrics collection       |
| 1.7-E2E-003  | E2E         | P1       | Complete webhook monitoring pipeline | Full n8n observability validation |

### AC4: Error Tracking - Edge Functions report errors to Sentry

#### Scenarios

| ID           | Level       | Priority | Test                                  | Justification                        |
| ------------ | ----------- | -------- | ------------------------------------- | ------------------------------------ |
| 1.7-UNIT-006 | Unit        | P0       | Sentry initialization in Deno         | Error tracking foundation            |
| 1.7-UNIT-007 | Unit        | P0       | Error capture and context injection   | Core error reporting functionality   |
| 1.7-INT-008  | Integration | P0       | Sentry error transmission             | External error reporting service     |
| 1.7-E2E-004  | E2E         | P0       | Edge Function error tracking pipeline | Complete error monitoring validation |

### AC5: Log Correlation - Logs include trace IDs for correlation

#### Scenarios

| ID           | Level       | Priority | Test                               | Justification                       |
| ------------ | ----------- | -------- | ---------------------------------- | ----------------------------------- |
| 1.7-UNIT-008 | Unit        | P1       | Winston trace ID injection         | Log correlation implementation      |
| 1.7-INT-009  | Integration | P1       | Cross-service trace ID propagation | Multi-service log correlation       |
| 1.7-E2E-005  | E2E         | P1       | End-to-end log trace correlation   | Complete observability verification |

### AC6: Alerting - Basic alerts configured for high error rates and slow queries

#### Scenarios

| ID          | Level       | Priority | Test                                | Justification              |
| ----------- | ----------- | -------- | ----------------------------------- | -------------------------- |
| 1.7-INT-010 | Integration | P2       | Prometheus alert rule configuration | Alert system foundation    |
| 1.7-E2E-006 | E2E         | P2       | Alert triggering and notification   | Alert system functionality |

### AC7: Documentation - Setup and troubleshooting guides completed

#### Scenarios

| ID         | Level  | Priority | Test                                    | Justification                  |
| ---------- | ------ | -------- | --------------------------------------- | ------------------------------ |
| 1.7-P2-001 | Manual | P2       | Documentation completeness verification | Knowledge transfer requirement |
| 1.7-P2-002 | Manual | P2       | Setup guide walkthrough validation      | Operational readiness          |

## Risk Coverage

### High-Risk Scenarios (P0 Tests)

**Security Risks:**

- 1.7-UNIT-006, 1.7-INT-008: Sentry API key exposure prevention
- 1.7-E2E-004: Edge Function error data sanitization

**Performance Risks:**

- 1.7-INT-001: Trace collection overhead validation
- 1.7-INT-003: Database monitoring query performance
- 1.7-E2E-001: End-to-end tracing latency impact

**Reliability Risks:**

- 1.7-INT-002: OTLP endpoint availability and failover
- 1.7-INT-006: n8n workflow execution reliability
- 1.7-E2E-003: Webhook monitoring pipeline resilience

### Medium-Risk Scenarios (P1 Tests)

**Configuration Risks:**

- 1.7-UNIT-003: Environment variable misconfiguration
- 1.7-INT-005: Monitoring threshold accuracy
- 1.7-INT-007: Metrics collection endpoint configuration

**Integration Risks:**

- 1.7-INT-004: Database monitoring view data accuracy
- 1.7-INT-009: Cross-service trace propagation
- 1.7-E2E-002: Grafana dashboard integration

## Detailed Test Specifications

### P0 Critical Tests

#### 1.7-UNIT-001: OpenTelemetry SDK initialization

```typescript
describe('OpenTelemetry SDK Initialization', () => {
  test('should initialize SDK with correct configuration', () => {
    // Verify NodeSDK instantiation
    // Validate trace exporter configuration
    // Confirm metric reader setup
    // Check instrumentation loading
  });
});
```

#### 1.7-INT-001: Service-to-service trace propagation

```typescript
describe('Distributed Tracing', () => {
  test('should propagate trace context across services', async () => {
    // Start trace in Service A
    // Make HTTP request to Service B
    // Verify trace ID propagation
    // Confirm parent-child span relationships
  });
});
```

#### 1.7-E2E-001: End-to-end request trace collection

```typescript
describe('Complete Tracing Pipeline', () => {
  test('should collect traces from request to response', async () => {
    // Send HTTP request to application
    // Verify trace appears in Jaeger
    // Validate span hierarchy
    // Check trace completion
  });
});
```

### P1 High Priority Tests

#### 1.7-INT-003: pg_stat_statements data collection

```sql
-- Test database monitoring setup
SELECT
  COUNT(*) as statement_count,
  MAX(mean_exec_time) as max_mean_time,
  AVG(mean_exec_time) as avg_mean_time
FROM pg_stat_statements
WHERE calls > 0;
```

#### 1.7-E2E-002: Grafana dashboard metric display

```typescript
describe('Grafana Integration', () => {
  test('should display PostgreSQL metrics in dashboard', async () => {
    // Execute database operations
    // Query Grafana API for metrics
    // Verify metric accuracy
    // Check dashboard responsiveness
  });
});
```

### P2 Medium Priority Tests

#### 1.7-INT-010: Prometheus alert rule configuration

```yaml
# prometheus-alerts-test.yml
groups:
  - name: flrts-monitoring-test
    rules:
      - alert: HighErrorRate
        expr: rate(http_requests_total{status=~"5.."}[5m]) > 0.01
        for: 1m
        labels:
          severity: warning
```

## Test Environment Requirements

### Unit Tests

- **Runtime**: Node.js 22 LTS + TypeScript 5.6
- **Framework**: Jest with TypeScript support
- **Mocks**: OpenTelemetry SDK components, Winston logger
- **Dependencies**: None (pure unit testing)

### Integration Tests

- **Database**: PostgreSQL test instance with pg_stat_statements
- **Services**: Docker Compose test environment
- **External APIs**: Mock Sentry and OTLP endpoints
- **Network**: Local container networking

### E2E Tests

- **Infrastructure**: Full monitoring stack (Prometheus, Grafana, Jaeger)
- **Services**: Complete FLRTS application stack
- **Network**: Production-like configuration
- **Data**: Realistic test data sets

## Test Data Requirements

### Trace Test Data

```typescript
interface TestTraceData {
  traceId: string;
  spanId: string;
  operationName: string;
  duration: number;
  tags: Record<string, string>;
  logs: Array<{ timestamp: number; message: string }>;
}
```

### Database Performance Test Data

```sql
-- Generate test load for monitoring
INSERT INTO test_performance_table
SELECT generate_series(1, 10000), md5(random()::text), now();
```

### n8n Webhook Test Data

```json
{
  "webhook_payload": {
    "test_id": "monitoring-test-001",
    "timestamp": "2025-09-16T00:16:06Z",
    "data": { "items": [1, 2, 3, 4, 5] }
  }
}
```

## Recommended Execution Order

### Phase 1: Foundation Validation (P0 Unit Tests)

1. 1.7-UNIT-001: OpenTelemetry SDK initialization
2. 1.7-UNIT-002: Trace exporter configuration
3. 1.7-UNIT-006: Sentry initialization in Deno
4. 1.7-UNIT-007: Error capture and context injection

### Phase 2: Integration Verification (P0 Integration Tests)

1. 1.7-INT-001: Service-to-service trace propagation
2. 1.7-INT-002: OTLP HTTP endpoint connectivity
3. 1.7-INT-003: pg_stat_statements data collection
4. 1.7-INT-004: Monitoring views data accuracy
5. 1.7-INT-006: n8n workflow execution tracking
6. 1.7-INT-008: Sentry error transmission

### Phase 3: End-to-End Validation (P0 E2E Tests)

1. 1.7-E2E-001: End-to-end request trace collection
2. 1.7-E2E-004: Edge Function error tracking pipeline

### Phase 4: Secondary Features (P1 Tests)

1. All remaining P1 unit, integration, and E2E tests
2. Performance validation under load
3. Error scenario testing

### Phase 5: Polish and Documentation (P2 Tests)

1. Alert configuration testing
2. Documentation validation
3. Cleanup and optimization verification

## Success Criteria

### Test Coverage Requirements

- **P0 Tests**: 100% pass rate required for production deployment
- **P1 Tests**: 95% pass rate required for release candidate
- **P2 Tests**: 80% pass rate acceptable for initial release

### Performance Benchmarks

- Unit tests: <100ms execution time per test
- Integration tests: <5s execution time per test
- E2E tests: <30s execution time per test
- Full test suite: <10 minutes total execution time

### Quality Gates

- No P0 test failures allowed
- Maximum 1 P1 test failure with documented workaround
- P2 test failures documented but non-blocking

## Maintenance Strategy

### Test Stability

- Mock external dependencies to prevent flaky tests
- Use deterministic test data where possible
- Implement retry logic for network-dependent tests
- Regular test environment cleanup and reset

### Test Evolution

- Update tests when monitoring requirements change
- Add regression tests for production issues
- Maintain test documentation alongside feature documentation
- Regular review of test effectiveness and coverage
